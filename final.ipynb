{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea70859f",
   "metadata": {},
   "source": [
    "# 유투브 채널 수리키친 더보기란 웹 스크랩핑"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "47b2e119",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import threading\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from webdriver_manager.chrome import ChromeDriverManager\n",
    "from bs4 import BeautifulSoup as bs\n",
    "from selenium.webdriver.common.action_chains import ActionChains"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1cb88f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# selenimu으로 크롬드라이버 실행\n",
    "driver = webdriver.Chrome(\"c:/my_web/chromedriver/chromedriver.exe\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbaa3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 페이지 scroll을 내리는 함수 \n",
    "def scroll() :\n",
    "    try :\n",
    "        # 페이지 내 스크롤 높이 받아오기\n",
    "        last_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "        while True : # true일 동안만 돌리기 \n",
    "            # 임의의 페이지 로딩시간 정하기 ## pc환경에 따라 로딩시간 최적화를 통해 로딩시간 단축\n",
    "            pause_time = random.uniform(1,2) # 인자로 들어온 a~b 사이의 실수(float)를 반환\n",
    "            # 페이지 최하단까지 스크롤\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight)\")\n",
    "            time.sleep(pause_time) # 일정 시간동안 프로세스를 일시정지. ##실수단위로도 지정할 수 있어 정교한 시간 제어가 가능 \n",
    "            # 무한 스크롤 동작을 위해 살짝 위로 스크롤(i.e., 페이지를 위로 올렸다가 내리는 제스쳐)\n",
    "            driver.execute_script(\"window.scrollTo(0, document.documentElement.scrollHeight)\")\n",
    "            # 페이지 내 스크롤 높이 새로 받아오기\n",
    "            new_page_height = driver.execute_script(\"return document.documentElement.scrollHeight\")\n",
    "            # 스크롤을 완료한 경우(더이상 페이지 높이 변화가 없는 경우)\n",
    "            if new_page_height == last_page_height:\n",
    "                print(\"스크롤 완료\")\n",
    "                break\n",
    "            # 스크롤 완료하지 않은 경우, 최하단까지 스크롤\n",
    "            else:\n",
    "                last_page_height = new_page_height\n",
    "                \n",
    "    except Exception as e:\n",
    "        print(\"에러 발생: \", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9835e7eb",
   "metadata": {},
   "source": [
    "## 1. 각 동영상별 링크, 제목 스크랩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c9a5944",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 유투브 수리키친 채널 페이지 페이지 response\n",
    "driver.get(\"https://www.youtube.com/@suri3180/videos\")\n",
    "time.sleep(3)\n",
    "scroll()\n",
    "html = driver.page_source\n",
    "soup = bs(html, 'html.parser')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ca235cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scrap\n",
    "# box \n",
    "boxes = soup.select(\"ytd-rich-item-renderer\")\n",
    "print(len(boxes))\n",
    "\n",
    "box = boxes[0] # 292개 중 첫 번째 동영상에 해당되는 태그\n",
    "print(len(box))\n",
    "print(box)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b31ccac",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 모든 'a' 요소의 href 뽑아오기 ###\n",
    "links_titles = []\n",
    "for box in boxes :\n",
    "    for a in box.select('a') : # 첫 box에 해당하는 모든 'a'요소\n",
    "        title = a.text\n",
    "        if a.has_attr('href'): # has_attr : href의 존재여부 확인\n",
    "            link = 'https://www.youtube.com/' + a['href']\n",
    "    links_titles.append([link,title])\n",
    "\n",
    "# 스크랩한 데이터 df로 변환\n",
    "data = {'link' : [link for link, title in links_titles], 'title' : [title for link, title in links_titles]}\n",
    "df = pd.DataFrame(data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a94c33cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# csv 파일로 저장\n",
    "df.to_csv('c:/my_web/multi_project/semi/link_title.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d8a939",
   "metadata": {},
   "source": [
    "## 2. 각 링크별 재료 스크랩"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4250476c",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_ingres = []\n",
    "final_link = []\n",
    "for link, title in links_titles :\n",
    "    driver.get(link)\n",
    "    time.sleep(3)\n",
    "    scroll()\n",
    "    try :\n",
    "        # 더보기 버튼 나타날 때 까지 기다리기\n",
    "        more_button = WebDriverWait(driver, 10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager\\\n",
    "                                            /ytd-watch-flexy/div[5]/div[1]/div/div[2]/ytd-watch-metadata/\\\n",
    "                                            div/div[3]/div[1]/div/ytd-text-inline-expander/tp-yt-paper-button[1]')))\n",
    "        # ActionChains 클래스는 사용자가 마우스 또는 키보드로 수행하는 작업을 시뮬레이트하는 데 사용\n",
    "        # move_to_element() 메소드는 주어진 요소로 마우스 커서를 이동시키는 데 사용\n",
    "        # click() 메소드는 마우스 왼쪽 버튼을 클릭하는 데 사용\n",
    "        # perform() 메소드는 이러한 동작을 실행\n",
    "        # ActionChains(driver).move_to_element(more_button).click().perform() 코드는 \n",
    "            # driver 객체를 사용하여 more_button 요소로 마우스를 이동한 다음 해당 요소를 클릭\n",
    "        ActionChains(driver).move_to_element(more_button).click().perform() # 버튼 클릭\n",
    "        \n",
    "        # 버튼 클릭 후 새로운 데이터가 뜰 때 까지 기다리기\n",
    "        new_data = WebDriverWait(driver,10).until(\n",
    "            EC.presence_of_element_located((By.XPATH, '/html/body/ytd-app/div[1]/ytd-page-manager/ytd-watch-flexy/div[5]/div[1]/div/div[2] \\\n",
    "                                            /ytd-watch-metadata/div/div[3]/div[1]/div/ytd-text-inline-expander/yt-attributed-string')))\n",
    "    except :\n",
    "        print('새로운 데이터가 로드되지 않았습니다.')\n",
    "\n",
    "    html = driver.page_source\n",
    "    soup = bs(html, 'html.parser')\n",
    "    \n",
    "    # 재료 text 스크랩 하기\n",
    "    description = soup.select('div#description')[0]\n",
    "    ingres = description.select('span')[3].text\n",
    "    if ingres != '' :\n",
    "        ingres = ingres.split('\\n')\n",
    "        ingres = list(filter(lambda x : x != '', ingres))\n",
    "        final_ingres.append(ingres)\n",
    "        final_link.append(link)\n",
    "\n",
    "print(final_ingres)\n",
    "print(len(final_ingres))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "200df4c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# list타입을 numpy 배열로 변환\n",
    "ingres_arr = np.array(final_ingres)\n",
    "link_arr = np.array(final_link)\n",
    "\n",
    "# df로 변환\n",
    "df_link = pd.DataFrame(link_arr)\n",
    "df_ingres = pd.DataFrame(ingres_arr)\n",
    "\n",
    "# link DF와 ingres DF 합치기(concat)\n",
    "final_df = pd.concat([df_link, df_ingres], axis = 1)\n",
    "final_df.columns = ['link', 'ingredient']\n",
    "\n",
    "# csv 파일로 저장 \n",
    "final_df.to_csv('c:/my_web/multi_project/semi/final_df.csv', encoding = 'utf-8')\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08574214",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 저장한 두 df 불러오기\n",
    "ingres_ = pd.read_csv('c:/my_web/multi_project/semi/final_df.csv', encoding = 'utf-8')\n",
    "title_ = pd.read_csv('c:/my_web/multi_project/semi/link_title.csv', encoding = 'utf-8') \n",
    "\n",
    "# link를 기준으로 join해주기\n",
    "df_right_join = pd.merge(title_, ingres_, left_on = 'link', right_on = 'link', how = 'right')\n",
    "df_right_join.head() # 확인\n",
    "\n",
    "# 불필요한 칼럼은 빼기 \n",
    "really_final2 = df_right_join[['link', 'title', 'ingredient']]\n",
    "really_final2.head() # 확인\n",
    "\n",
    "# 저장\n",
    "really_final2.to_csv('c:/my_web/multi_project/semi/really_final2.csv', encoding = 'utf-8')\n",
    "\n",
    "# 확인 (index_col = 0 으로 불필요한 열은 제외시키기)\n",
    "really_final2_test = pd.read_csv('c:/my_web/multi_project/semi/really_final2.csv', encoding = 'utf-8', index_col = 0)\n",
    "really_final2_test.head() # 확인"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee59869",
   "metadata": {},
   "source": [
    "## 3. Thumbnail 스크랩  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b8c998",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "for_thumbnail = pd.read_csv('c:/my_web/multi_project/semi/really_final3.csv', encoding = 'cp949', index_col = 0)\n",
    "# for_thumbnail.head()\n",
    "\n",
    "\n",
    "# 썸네일 뽑아오기\n",
    "thumbnail_list = []\n",
    "for link in df2['link'] :\n",
    "    response = requests.get(link)\n",
    "    html = response.text\n",
    "    soup = bs(html, 'html.parser')\n",
    "    # 썸네일 이미지 태그 찾아 해당 태그의 src 속성 찾아오기\n",
    "    thumbnail = soup.find('link', {'itemprop' : 'thumbnailUrl'})\n",
    "    thumbnail_url = thumbnail.get('href')\n",
    "    thumbnail_list.append(thumbnail_url)\n",
    "thumbnail_list\n",
    "\n",
    "# df로 변환\n",
    "image = pd.DataFrame(data = thumbnail_list)\n",
    "image.columns = ['thumbnail']\n",
    "# image.head()\n",
    "\n",
    "# 기존 df와 병합(concat)\n",
    "really_final4 = pd.concat([df2, image], axis=1)\n",
    "# really_final4.head()\n",
    "\n",
    "# 저장 \n",
    "really_final4.to_csv('c:/my_web/multi_project/semi/really_final9.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7a54fb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 재료만 나오도록 전처리 \n",
    "df_semi_fnal = pd.read_csv('c:/my_web/multi_project/semi/really_final9.csv', encoding = 'ansi')\n",
    "\n",
    "df_semi_fnal[\"ingredient\"] = df_semi_fnal[\"ingredient\"].str.replace(\"[\", \"\")\n",
    "df_semi_fnal[\"ingredient\"] = df_semi_fnal[\"ingredient\"].str.replace(\"]\", \"\")\n",
    "df_semi_fnal['ingredient'] = df_semi_fnal['ingredient'].str.replace(\"'\", '')\n",
    "df_semi_fnal['ingredient'] = df_semi_fnal['ingredient'].replace(' ', '뭐먹지')\n",
    "# df_semi_fnal.iloc[232]['ingredient'] ## 확인용\n",
    "\n",
    "for i in range(len(df_semi_fnal)):\n",
    "    if \"재료,\" in df15.iloc[i]['ingredient']:\n",
    "        if \", *\" in df15.iloc[i]['ingredient']:\n",
    "            df15.iloc[i]['ingredient'] = df15.iloc[i]['ingredient'].split(\"재료,\")[1].split(\", *\")[0].strip()\n",
    "        elif \", ?\" in df15.iloc[i]['ingredient']:\n",
    "            df15.iloc[i]['ingredient'] = df15.iloc[i]['ingredient'].split(\"재료,\")[1].split(\", ?\")[0].strip()\n",
    "        else:\n",
    "            df15.iloc[i]['ingredient'] = df15.iloc[i]['ingredient'].split(\"재료,\")[1].strip()\n",
    "    else:\n",
    "        continue\n",
    "        \n",
    "df_semi_fnal.to_csv('c:/my_web/multi_project/semi/really_final9.csv', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
